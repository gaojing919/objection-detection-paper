YOLO9000:Better, Faster, Stronger   
[http://blog.csdn.net/jesse_mx/article/details/53925356](http://blog.csdn.net/jesse_mx/article/details/53925356)   
[https://zhuanlan.zhihu.com/p/25167153](https://zhuanlan.zhihu.com/p/25167153)
# 摘要 #
YOLO9000可以检测9000个不同类别的物体  
YOLOv2 可以在不同的尺度上运行  
在VOC 2007	数据集，67FPS的速度mAP是76.8，40FPS的速度mAP是78.6 
# 1.介绍 #
大多数物体检测方法受约束于一个很小的物体集  
分类的数据集比检测的数据集更广，想让检测可以达到分类的数据水平，但标记数据用于检测很难，拿到跟分类数据一样多的检测数据在近期是不可能的   
利用大量的分类数据：物体分类的分层视角，结合不同的数据集  
联合训练方法：在检测和分类数据上训练物体检测器，检测数据用来学习精确的定位物体，分类数据用来增加词汇量和鲁棒性  
对YOLO的提升是YOLOv2,然后使用结合数据集和联合训练的方法，训练一个可以检测9000多种类别的模型叫做YOLO9000,用的数据集是分类的9000类ImageNet和检测的COCO   
代码位置：[http://pjreddie.com/yolo9000/](http://pjreddie.com/yolo9000/) 
# 2.Better #
主要改进YOLO定位错误和底的召回率，同时维持分类精度  
**批规范化**：在收敛上有很大的提升，不需要其他的正则化方法，给所有的卷积层都加上批规范化，提高2%mAP,移除了dropout也不会导致正则化  
BN的具体情况：[http://blog.csdn.net/hjimce/article/details/50866313](http://blog.csdn.net/hjimce/article/details/50866313)   
**高分辨率分类**：原来的YOLO是在Image net用\\(224\times 224\\)的分辨率上训练分类网络，然后增加分辨率到\\(448\times 448\\)用于检测  
YOLOv2首先使用10次\\(448\times 448\\)的Image net图片微调分类网络，然后再微调用于检测  
提高mAP4%  
**Convolutional With Anchor Boxes**：移除YOLO的全连接层，使用Anchor Boxes预测Bounding box ,首先消除一个池化层，让卷积层有更高的分辨率，把\\(448\times 448\\)（下采样因子是32，feature map是\\(14\times 14\\)）的图片缩小成\\(416\times416\\)（下采样因子是32，feature map是\\(13\times 13\\)）,使得feature map的位置数为奇数，这样就有一个唯一的中心  
通过在卷积层使用anchor boxes，网络可以预测超过1000个窗口，使用Anchor Boxes 精确率是降低，但是recall会从81%上涨到88%  
**维度聚类**：
之前Anchor Box的尺寸是手动选择的，所以尺寸还有优化的余地。 为了优化，在训练集（training set）Bounding Boxes上跑了一下k-means聚类，来找到一个比较好的值。   
如果我们用标准的欧式距离的k-means，尺寸大的框比小框产生更多的错误。因为我们的目的是提高IOU分数，这依赖于Box的大小，所以距离度量的使用： 
![](https://i.imgur.com/Jcqz2Wp.png)   
**直接位置预测**  
模型不稳定  
最终，网络在特征图（13 *13 ）的每个cell上预测5个bounding boxes，每一个bounding box预测5个坐标值：tx，ty，tw，th，to。如果这个cell距离图像左上角的边距为（cx，cy）以及该cell对应的box维度（bounding box prior）的长和宽分别为（pw，ph），那么对应的box为：    
![](https://i.imgur.com/UJCr20k.png)  
使用Dimension Clusters和Direct location prediction这两项anchor boxes改进方法，mAP获得了5%的提升  
**细粒度特征**   
修改后的网络最终在13 * 13的特征图上进行预测，虽然这足以胜任大尺度物体的检测，如果用上细粒度特征的话可能对小尺度的物体检测有帮助。Faser R-CNN和SSD都在不同层次的特征图上产生区域建议以获得多尺度的适应性。YOLOv2使用了一种不同的方法，简单添加一个 passthrough layer，把浅层特征图（分辨率为26 * 26）连接到深层特征图。  
passthroughlaye把高低分辨率的特征图做连结，叠加相邻特征到不同通道（而非空间位置）
，类似于Resnet中的identity mappings。这个方法把26 * 26 * 512的特征图叠加成13 * 13 * 2048的特征图，与原生的深层特征图相连接。  
YOLOv2的检测器使用的就是经过扩展后的的特征图，它可以使用细粒度特征，使得模型的性能获得了1%的提升。  
**多尺度训练**  
每经过10批训练（10 batches）就会随机选择新的图片尺寸。网络使用的降采样参数为32，于是使用32的倍数{320,352，…，608}，最小的尺寸为320 * 320，最大的尺寸为608 * 608。 调整网络到相应维度然后继续进行训练。  
**进一步的实验**   
![](https://i.imgur.com/6aB9MkB.png)  
**发现问题：boat,bottle,chair,table,plant的识别精确度都很低，所有的方法中，这5个物体的识别率都低于平均水平10到20个点，相比较而言，SSD对这5种物体的识别是最好的**  
# 3.更快 #
大多数检测网络依赖于VGG-16作为特征提取网络，VGG-16是一个强大而准确的分类网络，但是确过于复杂。224 * 224的图片进行一次前向传播，其卷积层就需要多达306.9亿次浮点数运算  
YOLO使用的是基于Googlenet的自定制网络，比VGG-16更快，一次前向传播仅需85.2亿次运算，不过它的精度要略低于VGG-16。224 * 224图片取 single-crop, top-5 accuracy，YOLO的定制网络得到88%（VGG-16得到90%）。  
**Darknet-19**  
类似于VGG，网络使用了较多的3 * 3卷积核，在每一次池化操作后把通道数翻倍。  
借鉴了network in network的思想，网络使用了全局平均池化（global average pooling）做预测，把1 * 1的卷积核置于3 * 3的卷积核之间，用来压缩特征。使用batch normalization稳定模型训练，加速收敛，正则化模型。  
最终得出的基础模型就是Darknet-19，包含19个卷积层、5个最大值池化层（max pooling layers ）。Darknet-19处理一张照片需要55.8亿次运算，imagenet的top-1准确率为72.9%，top-5准确率为91.2%    
**Training for classiﬁcation**  
作者使用Darknet-19在标准1000类的ImageNet上训练了160次，用随机梯度下降法，starting learning rate 为0.1，polynomial rate decay 为4，weight decay为0.0005 ，momentum 为0.9。训练的时候仍然使用了很多常见的数据扩充方法（data augmentation），包括random crops, rotations, and hue, saturation, and exposure shifts。（参数都是基于作者的darknet框架）  
初始的224 * 224训练后把分辨率上调到了448 * 448，使用同样的参数又训练了10次，学习率调整到了10^{-3}。高分辨率下训练的分类网络top-1准确率76.5%，top-5准确率93.3%。  
**Training for detection**  
为了把分类网络改成检测网络，去掉原网络最后一个卷积层，增加了三个 3 * 3 （1024 ﬁlters）的卷积层，并且在每一个卷积层后面跟一个1 * 1的卷积层，输出维度是检测所需数量。  
对于VOC数据集，预测5种boxes，每个box包含5个坐标值和20个类别，所以总共是5 * （5+20）= 125个输出维度。  
也添加了passthrough layer，从最后3 * 3 * 512的卷积层连到倒数第二层，使模型有了细粒度特征。  
学习策略是：先以10^{-3}的初始学习率训练了160次，在第60次和第90次的时候学习率减为原来的十分之一。weight decay为0.0005，momentum为0.9，以及类似于Faster-RCNN和SSD的数据扩充（data augmentation）策略： random crops, color shifting, etc。使用相同的策略在 COCO 和VOC上训练。  
# 4.更强大 #
作者提出了一种在分类数据集和检测数据集上联合训练的机制。使用检测数据集的图片去学习检测相关的信息，例如bounding box 坐标预测，是否包含物体以及属于各个物体的概率。使用仅有类别标签的分类数据集图片去扩展可以检测的种类。  
训练过程中把监测数据和分类数据混合在一起。当网络遇到一张属于检测数据集的图片就基于YOLOv2的全部损失函数（包含分类部分和检测部分）做反向传播。当网络遇到一张属于分类数据集的图片就仅基于分类部分的损失函数做反向传播。  
这种方法有一些难点需要解决。检测数据集只有常见物体和抽象标签（不具体），例如 “狗”，“船”。分类数据集拥有广而深的标签范围（例如ImageNet就有一百多类狗的品种，包括 “Norfolk terrier”, “Yorkshire terrier”, and “Bedlington terrier”等. ）。必须按照某种一致的方式来整合两类标签。  
大多数分类的方法采用softmax层，考虑所有可能的种类计算最终的概率分布。但是softmax假设类别之间互不包含，但是整合之后的数据是类别是有包含关系的，例如 “Norfolk terrier” 和 “dog”。 所以整合数据集没法使用这种方式（softmax 模型），  
**Hierarchical classiﬁcation（层次式分类）**  
WordNet是一个有向图结构（而非树结构），为了简化问题，作者从ImageNet的概念中构建了一个层次树结构（hierarchical tree）来代替图结构方案。  
创建层次树的步骤是：   
1. 遍历ImageNet的所有视觉名词  
2. 对每一个名词，在WordNet上找到从它所在位置到根节点（“physical object”）的路径。 许多同义词集只有一条路径。所以先把这些路径加入层次树结构。  
3. 然后迭代检查剩下的名词，得到路径，逐个加入到层次树。路径选择办法是：如果一个名词有两条路径到根节点，其中一条需要添加3个边到层次树，另一条仅需添加一条边，那么就选择添加边数少的那条路径。
最终结果是一颗 WordTree （视觉名词组成的层次结构模型）。用WordTree执行分类时，预测每个节点的条件概率。例如： 在“terrier”节点会预测：      
![](https://i.imgur.com/LV1opH3.png)  
如果想求得特定节点的绝对概率，只需要沿着路径做连续乘积。例如 如果想知道一张图片是不是“Norfolk terrier ”需要计算：  
![](https://i.imgur.com/qg5ijVe.png)  
分类时假设 图片包含物体：Pr(physical object) = 1.  
为了验证这种方法作者在WordTree（用1000类别的ImageNet创建）上训练了Darknet-19模型。为了创建WordTree1k作者天添加了很多中间节点，把标签由1000扩展到1369。训练过程中ground truth标签要顺着向根节点的路径传播：例如 如果一张图片被标记为“Norfolk terrier”它也被标记为“dog” 和“mammal”等。为了计算条件概率，模型预测了一个包含1369个元素的向量，而且基于所有“同义词集”计算softmax，其中“同义词集”是同一概念的下位词。  
使用相同的训练参数，层次式Darknet-19获得71.9%的top-1精度和90.4%top-5精度。尽管添加了369个额外概念，且让网络去预测树形结构，精度只有略微降低。按照这种方式执行分类有一些好处，当遇到新的或未知物体类别，预测精确度降低的很温和（没有突然巨幅下降）。例如：如果网络看到一张狗的图片，但是不确定狗的类别，网络预测为狗的置信度依然很高，但是，狗的下位词（“xx狗”）的置信度就比较低。    
这个策略野同样可用于检测。不在假设每一张图片都包含物体，取而代之使用YOLOv2的物体预测器（objectness predictor）得到Pr(physical object)的值。检测器预测一个bounding box和概率树（WordTree）。沿着根节点向下每次都走置信度最高的分支直到达到某个阈值，最终预测物体的类别为最后的节点类别。  
**问题：那有没有再用Image net预训练去初始化网络，如果预训练了的话，那相当于使用相同的数据训练了两次，是不是有冗余**  
**Dataset combination with WordTree**  
可以使用WordTree把多个数据集整合在一起。只需要把数据集中的类别映射到树结构中的同义词集合（synsets）。使用WordTree整合ImageNet和COCO的标签  
**joint classification and detection(联合训练分类和检测)**  
使用WordTree整合了数据集之后就可以在数据集（分类-检测数据）上训练联合模型。我们想要训练一个检测类别很大的检测器所以使用COCO检测数据集和全部ImageNet的前9000类创造一个联合数据集。为了评估我们使用的方法，也从ImageNet detection challenge 中向整合数据集添加一些还没有存在于整合数据集的类别。相应的WordTree有9418个类别。由于ImageNet是一个非常大的数据集，所以通过oversampling COCO数据集来保持平衡，使ImageNet：COCO = 4：1。  
anchor box数量由5调整为3用以限制输出大小。  
当网络遇到一张分类图片仅反向传播分类损失。在该类别对应的所有bounding box中找到一个置信度最高的（作为预测坐标），同样只反向传播该类及其路径以上对应节点的类别损失。反向传播objectness损失基于如下假设：预测box与ground truth box的重叠度至少0.31IOU。    
作者在ImageNet detection task上评估YOLO9000。ImageNet detection task和COCO有44个物体类别是相同的。这意味着YOLO9000只从大多数测试数据集中看到过分类数据而非检测数据。最终整体精度为19.7mAP，在从未见过的156个物体检测数据类别上精度为16.0mAP。这个结果高于DPM，但是YOLO9000是在不同数据集上进行半监督训练。而且YOLO9000可以同时实时检测9000多种其它物体类别。  
作者也分析了YOLO9000在ImageNet上的性能，发现可以学习新的动物表现很好，但是学习衣服和设备这类物体则不行。因为从COCO数据集上动物类别那里学习到的物体预测泛化性很好。但是COCO数据集并没有任何衣服类别的标签数据（只有"人"类别），所以YOLO9000很难对“太阳镜”，“游泳裤”这些类别建模。   